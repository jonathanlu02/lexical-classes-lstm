{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import keras\n",
    "from pickle import dump\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Jonathan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Jonathan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Jonathan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run once\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Reading in the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file):\n",
    "    '''\n",
    "    Read each text file into a string\n",
    "    '''\n",
    "    f = open(file, 'r', encoding='utf8')\n",
    "    text = f.read()\n",
    "    f.close()\n",
    "    return text\n",
    "\n",
    "def folder_list(path):\n",
    "    '''\n",
    "    Reads each text file in a folder and concatenates each file into a bigger string\n",
    "    Parameter 'path' is the path of your local folder.\n",
    "    '''\n",
    "    filelist = os.listdir(path)\n",
    "    text = ''\n",
    "    for infile in filelist:\n",
    "        file = os.path.join(path,infile)\n",
    "        text_data = read_data(file)\n",
    "        text += '\\n ' + text_data\n",
    "    return text\n",
    "\n",
    "def load_and_shuffle_data():\n",
    "    path = \"data/\"\n",
    "\n",
    "    text = folder_list(path)\n",
    "    #print(text)\n",
    "    #random.shuffle(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_and_shuffle_data()\n",
    "#print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Expand Contractions\n",
    "example: I'm -> I am"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTRACTION_MAP = { \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"am not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how does\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'd've\": \"i would have\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'll've\": \"i will have\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so is\",\n",
    "\"that'd\": \"that had\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        #print(match)\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())\n",
    "        #print(expanded_contraction)\n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: it is I will I am have not\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "print(expand_contractions('''test: it's I'll I'm haven't''',CONTRACTION_MAP))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Remove non-alphabetical characters\n",
    "Remove special characters like periods, commas, as well as digits. Only keeping alphabet characters and spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_alphabetical_characters(text):\n",
    "    return re.sub(r'[^a-zA-Z\\s]', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t e s t\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "print(remove_non_alphabetical_characters('t !@#$%^&*()_+-={}[]:\"e;''\">? s ./\\|<>,t1234567890`'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Removing stopwords\n",
    "example: remove 'a', 'the', 'and', etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    stopword_list = set(stopwords.words('english')) \n",
    "    \n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    \n",
    "    filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    \n",
    "    return ' '.join(filtered_tokens) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test test test test test\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "print(remove_stopwords('this test is a test to test the test from a test'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Stemming \n",
    "#### (may remove due to aggressive pruning issue + lemmatization being more effective)\n",
    "example: making -> make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemmer_text(text):\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    text = ' '.join([ps.stem(word) for word in text.split()])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make make make\n",
      "tri tri poni us\n"
     ]
    }
   ],
   "source": [
    "print(stemmer_text('making make makes'))\n",
    "print(stemmer_text('try tries ponies us')) # issue with stemming - https://medium.com/@tusharsri/nlp-a-quick-guide-to-stemming-60f1ca5db49e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e. Lemmatization\n",
    "example: tries -> try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    wn_lemmatizer = WordNetLemmatizer()\n",
    "    text = ' '.join([wn_lemmatizer.lemmatize(word) for word in text.split()])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making make make\n",
      "try try pony u\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "print(lemmatize_text('making make makes'))\n",
    "print(lemmatize_text('try tries ponies us'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    #text = [j for sub in text for j in sub]  # flatten list of all books into 1d list\n",
    "    #text = [word.lower() for word in text]\n",
    "    text = expand_contractions(text)\n",
    "    text = remove_non_alphabetical_characters(text)\n",
    "    text = remove_stopwords(text)\n",
    "    #text = stemmer_text(text)\n",
    "    text = lemmatize_text(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_text = preprocess(data)\n",
    "#print(preprocessed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 243801\n",
      "Number of unique words: 18023\n"
     ]
    }
   ],
   "source": [
    "tokens = preprocessed_text.split()\n",
    "print('Number of words: ' +str(len(tokens)))\n",
    "print('Number of unique words: ' +str(len(set(tokens))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Sequence of words + integer encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 243750\n"
     ]
    }
   ],
   "source": [
    "# Load all tokens into sequences (so that model can learn to predict words after a given sequence)\n",
    "# see https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/\n",
    "\n",
    "# organize into sequences of tokens\n",
    "length = 50 + 1\n",
    "sequences = list()\n",
    "for i in range(length, len(tokens)):\n",
    "    # select sequence of tokens\n",
    "    seq = tokens[i-length:i]\n",
    "    # convert into a line\n",
    "    line = ' '.join(seq)\n",
    "    # store\n",
    "    sequences.append(line)\n",
    "print('Total Sequences: %d' % len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save tokens to file, one dialog per line\n",
    "def save_doc(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "\n",
    "out_filename = 'sequences50.txt'\n",
    "save_doc(sequences, out_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "# used to save the keras model every 10 epochs\n",
    "class CustomSaver(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % 10 == 0:  # or save after some epoch, each k-th epoch etc.\n",
    "            self.model.save(\"model_{}.hd5\".format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    " \n",
    "# load\n",
    "in_filename = 'sequences50.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer encode sequences of words\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines)\n",
    "# vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    " \n",
    "# separate into input and output\n",
    "sequences = np.array(sequences)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "seq_length = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 50, 50)            901200    \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, 50, 100)           60400     \n",
      "_________________________________________________________________\n",
      "lstm_12 (LSTM)               (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 18024)             1820424   \n",
      "=================================================================\n",
      "Total params: 2,872,524\n",
      "Trainable params: 2,872,524\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jonathan\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 195000 samples, validate on 48750 samples\n",
      "Epoch 1/100\n",
      "195000/195000 [==============================] - 406s 2ms/step - loss: 8.1087 - accuracy: 0.0098 - val_loss: 8.4298 - val_accuracy: 0.0121\n",
      "Epoch 2/100\n",
      "195000/195000 [==============================] - 386s 2ms/step - loss: 7.8475 - accuracy: 0.0123 - val_loss: 8.4770 - val_accuracy: 0.0110\n",
      "Epoch 3/100\n",
      "195000/195000 [==============================] - 402s 2ms/step - loss: 7.6433 - accuracy: 0.0167 - val_loss: 8.5538 - val_accuracy: 0.0105\n",
      "Epoch 4/100\n",
      "195000/195000 [==============================] - 390s 2ms/step - loss: 7.4510 - accuracy: 0.0219 - val_loss: 8.6428 - val_accuracy: 0.0108\n",
      "Epoch 5/100\n",
      "195000/195000 [==============================] - 457s 2ms/step - loss: 7.2760 - accuracy: 0.0266 - val_loss: 8.7487 - val_accuracy: 0.0119\n",
      "Epoch 6/100\n",
      "195000/195000 [==============================] - 461s 2ms/step - loss: 7.0942 - accuracy: 0.0309 - val_loss: 8.8569 - val_accuracy: 0.0107\n",
      "Epoch 7/100\n",
      "195000/195000 [==============================] - 477s 2ms/step - loss: 6.9354 - accuracy: 0.0340 - val_loss: 8.9896 - val_accuracy: 0.0104\n",
      "Epoch 8/100\n",
      "195000/195000 [==============================] - 454s 2ms/step - loss: 6.7902 - accuracy: 0.0363 - val_loss: 9.2192 - val_accuracy: 0.0111\n",
      "Epoch 9/100\n",
      "195000/195000 [==============================] - 443s 2ms/step - loss: 6.6538 - accuracy: 0.0382 - val_loss: 9.5155 - val_accuracy: 0.0088\n",
      "Epoch 10/100\n",
      "195000/195000 [==============================] - 463s 2ms/step - loss: 6.5218 - accuracy: 0.0403 - val_loss: 9.8589 - val_accuracy: 0.0102\n",
      "Epoch 11/100\n",
      "195000/195000 [==============================] - 460s 2ms/step - loss: 6.3926 - accuracy: 0.0426 - val_loss: 10.1928 - val_accuracy: 0.0099\n",
      "Epoch 12/100\n",
      "195000/195000 [==============================] - 456s 2ms/step - loss: 6.2683 - accuracy: 0.0453 - val_loss: 10.6158 - val_accuracy: 0.0089\n",
      "Epoch 13/100\n",
      "195000/195000 [==============================] - 424s 2ms/step - loss: 6.1477 - accuracy: 0.0475 - val_loss: 11.1799 - val_accuracy: 0.0085\n",
      "Epoch 14/100\n",
      "195000/195000 [==============================] - 419s 2ms/step - loss: 6.0266 - accuracy: 0.0504 - val_loss: 11.7720 - val_accuracy: 0.0079\n",
      "Epoch 15/100\n",
      "195000/195000 [==============================] - 458s 2ms/step - loss: 5.9054 - accuracy: 0.0542 - val_loss: 12.3090 - val_accuracy: 0.0072\n",
      "Epoch 16/100\n",
      "195000/195000 [==============================] - 457s 2ms/step - loss: 5.7888 - accuracy: 0.0593 - val_loss: 13.0263 - val_accuracy: 0.0072\n",
      "Epoch 17/100\n",
      "195000/195000 [==============================] - 453s 2ms/step - loss: 5.6753 - accuracy: 0.0650 - val_loss: 13.7597 - val_accuracy: 0.0059\n",
      "Epoch 18/100\n",
      "195000/195000 [==============================] - 449s 2ms/step - loss: 5.5660 - accuracy: 0.0713 - val_loss: 14.2914 - val_accuracy: 0.0058\n",
      "Epoch 19/100\n",
      "195000/195000 [==============================] - 448s 2ms/step - loss: 5.4610 - accuracy: 0.0791 - val_loss: 15.2204 - val_accuracy: 0.0057\n",
      "Epoch 20/100\n",
      "195000/195000 [==============================] - 440s 2ms/step - loss: 5.3579 - accuracy: 0.0875 - val_loss: 16.1533 - val_accuracy: 0.0051\n",
      "Epoch 21/100\n",
      "195000/195000 [==============================] - 426s 2ms/step - loss: 5.2605 - accuracy: 0.0974 - val_loss: 16.5418 - val_accuracy: 0.0047\n",
      "Epoch 22/100\n",
      "195000/195000 [==============================] - 434s 2ms/step - loss: 5.1663 - accuracy: 0.1072 - val_loss: 17.2431 - val_accuracy: 0.0040\n",
      "Epoch 23/100\n",
      "195000/195000 [==============================] - 426s 2ms/step - loss: 5.0764 - accuracy: 0.1163 - val_loss: 17.9868 - val_accuracy: 0.0038\n",
      "Epoch 24/100\n",
      "195000/195000 [==============================] - 453s 2ms/step - loss: 4.9911 - accuracy: 0.1260 - val_loss: 18.5304 - val_accuracy: 0.0042\n",
      "Epoch 25/100\n",
      "195000/195000 [==============================] - 456s 2ms/step - loss: 4.9100 - accuracy: 0.1355 - val_loss: 19.1928 - val_accuracy: 0.0043\n",
      "Epoch 26/100\n",
      "195000/195000 [==============================] - 491s 3ms/step - loss: 4.8293 - accuracy: 0.1444 - val_loss: 19.8816 - val_accuracy: 0.0038\n",
      "Epoch 27/100\n",
      "195000/195000 [==============================] - 467s 2ms/step - loss: 4.7550 - accuracy: 0.1525 - val_loss: 20.5217 - val_accuracy: 0.0034\n",
      "Epoch 28/100\n",
      "195000/195000 [==============================] - 492s 3ms/step - loss: 4.6855 - accuracy: 0.1611 - val_loss: 21.1113 - val_accuracy: 0.0035\n",
      "Epoch 29/100\n",
      "195000/195000 [==============================] - 495s 3ms/step - loss: 4.6131 - accuracy: 0.1693 - val_loss: 21.6984 - val_accuracy: 0.0037\n",
      "Epoch 30/100\n",
      "195000/195000 [==============================] - 477s 2ms/step - loss: 4.5485 - accuracy: 0.1767 - val_loss: 21.8127 - val_accuracy: 0.0034\n",
      "Epoch 31/100\n",
      "195000/195000 [==============================] - 419s 2ms/step - loss: 4.4867 - accuracy: 0.1847 - val_loss: 22.8469 - val_accuracy: 0.0034\n",
      "Epoch 32/100\n",
      "195000/195000 [==============================] - 416s 2ms/step - loss: 4.4233 - accuracy: 0.1915 - val_loss: 23.0093 - val_accuracy: 0.0033\n",
      "Epoch 33/100\n",
      "195000/195000 [==============================] - 418s 2ms/step - loss: 4.3634 - accuracy: 0.1993 - val_loss: 23.9593 - val_accuracy: 0.0035\n",
      "Epoch 34/100\n",
      "195000/195000 [==============================] - 425s 2ms/step - loss: 4.3047 - accuracy: 0.2071 - val_loss: 24.0509 - val_accuracy: 0.0033\n",
      "Epoch 35/100\n",
      "195000/195000 [==============================] - 421s 2ms/step - loss: 4.2488 - accuracy: 0.2136 - val_loss: 24.6312 - val_accuracy: 0.0035\n",
      "Epoch 36/100\n",
      "195000/195000 [==============================] - 416s 2ms/step - loss: 4.1923 - accuracy: 0.2221 - val_loss: 24.9650 - val_accuracy: 0.0035\n",
      "Epoch 37/100\n",
      "195000/195000 [==============================] - 440s 2ms/step - loss: 4.1411 - accuracy: 0.2278 - val_loss: 25.3841 - val_accuracy: 0.0035\n",
      "Epoch 38/100\n",
      "195000/195000 [==============================] - 471s 2ms/step - loss: 4.0863 - accuracy: 0.2347 - val_loss: 25.7132 - val_accuracy: 0.0031\n",
      "Epoch 39/100\n",
      "195000/195000 [==============================] - 424s 2ms/step - loss: 4.0381 - accuracy: 0.2411 - val_loss: 26.3773 - val_accuracy: 0.0033\n",
      "Epoch 40/100\n",
      "195000/195000 [==============================] - 472s 2ms/step - loss: 3.9899 - accuracy: 0.2474 - val_loss: 26.5213 - val_accuracy: 0.0032\n",
      "Epoch 41/100\n",
      "195000/195000 [==============================] - 427s 2ms/step - loss: 3.9409 - accuracy: 0.2549 - val_loss: 27.5289 - val_accuracy: 0.0031\n",
      "Epoch 42/100\n",
      "195000/195000 [==============================] - 502s 3ms/step - loss: 3.8901 - accuracy: 0.2618 - val_loss: 27.5204 - val_accuracy: 0.0031\n",
      "Epoch 43/100\n",
      "195000/195000 [==============================] - 426s 2ms/step - loss: 3.8455 - accuracy: 0.2666 - val_loss: 28.4874 - val_accuracy: 0.0029\n",
      "Epoch 44/100\n",
      "195000/195000 [==============================] - 405s 2ms/step - loss: 3.8052 - accuracy: 0.2730 - val_loss: 28.7284 - val_accuracy: 0.0031\n",
      "Epoch 45/100\n",
      "195000/195000 [==============================] - 418s 2ms/step - loss: 3.7623 - accuracy: 0.2791 - val_loss: 29.2161 - val_accuracy: 0.0029\n",
      "Epoch 46/100\n",
      "195000/195000 [==============================] - 394s 2ms/step - loss: 3.7159 - accuracy: 0.2854 - val_loss: 29.4016 - val_accuracy: 0.0028\n",
      "Epoch 47/100\n",
      "195000/195000 [==============================] - 454s 2ms/step - loss: 3.6781 - accuracy: 0.2899 - val_loss: 29.6402 - val_accuracy: 0.0029\n",
      "Epoch 48/100\n",
      "195000/195000 [==============================] - 394s 2ms/step - loss: 3.6355 - accuracy: 0.2968 - val_loss: 29.9427 - val_accuracy: 0.0025\n",
      "Epoch 49/100\n",
      "195000/195000 [==============================] - 414s 2ms/step - loss: 3.5984 - accuracy: 0.3035 - val_loss: 30.5759 - val_accuracy: 0.0031\n",
      "Epoch 50/100\n",
      "195000/195000 [==============================] - 402s 2ms/step - loss: 3.5594 - accuracy: 0.3078 - val_loss: 31.3744 - val_accuracy: 0.0026\n",
      "Epoch 51/100\n",
      "195000/195000 [==============================] - 479s 2ms/step - loss: 3.5263 - accuracy: 0.3123 - val_loss: 30.8654 - val_accuracy: 0.0026\n",
      "Epoch 52/100\n",
      "195000/195000 [==============================] - 412s 2ms/step - loss: 3.4891 - accuracy: 0.3177 - val_loss: 31.8116 - val_accuracy: 0.0029\n",
      "Epoch 53/100\n",
      "195000/195000 [==============================] - 417s 2ms/step - loss: 3.4505 - accuracy: 0.3236 - val_loss: 31.5427 - val_accuracy: 0.0027\n",
      "Epoch 54/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195000/195000 [==============================] - 408s 2ms/step - loss: 3.4174 - accuracy: 0.3298 - val_loss: 32.3963 - val_accuracy: 0.0029\n",
      "Epoch 55/100\n",
      "195000/195000 [==============================] - 407s 2ms/step - loss: 3.3849 - accuracy: 0.3337 - val_loss: 32.6961 - val_accuracy: 0.0030\n",
      "Epoch 56/100\n",
      "195000/195000 [==============================] - 419s 2ms/step - loss: 3.3543 - accuracy: 0.3381 - val_loss: 33.1385 - val_accuracy: 0.0026\n",
      "Epoch 57/100\n",
      "195000/195000 [==============================] - 403s 2ms/step - loss: 3.3231 - accuracy: 0.3431 - val_loss: 33.2528 - val_accuracy: 0.0026\n",
      "Epoch 58/100\n",
      "195000/195000 [==============================] - 423s 2ms/step - loss: 3.2931 - accuracy: 0.3474 - val_loss: 33.7993 - val_accuracy: 0.0027\n",
      "Epoch 59/100\n",
      "195000/195000 [==============================] - 401s 2ms/step - loss: 3.2618 - accuracy: 0.3517 - val_loss: 34.0325 - val_accuracy: 0.0027\n",
      "Epoch 60/100\n",
      "195000/195000 [==============================] - 463s 2ms/step - loss: 3.2345 - accuracy: 0.3564 - val_loss: 34.1748 - val_accuracy: 0.0027\n",
      "Epoch 61/100\n",
      "195000/195000 [==============================] - 403s 2ms/step - loss: 3.2053 - accuracy: 0.3603 - val_loss: 34.6093 - val_accuracy: 0.0030\n",
      "Epoch 62/100\n",
      "195000/195000 [==============================] - 434s 2ms/step - loss: 3.1776 - accuracy: 0.3648 - val_loss: 35.7380 - val_accuracy: 0.0026\n",
      "Epoch 63/100\n",
      "195000/195000 [==============================] - 409s 2ms/step - loss: 3.1493 - accuracy: 0.3693 - val_loss: 35.1084 - val_accuracy: 0.0028\n",
      "Epoch 64/100\n",
      "195000/195000 [==============================] - 408s 2ms/step - loss: 3.1250 - accuracy: 0.3737 - val_loss: 35.9590 - val_accuracy: 0.0025\n",
      "Epoch 65/100\n",
      "195000/195000 [==============================] - 412s 2ms/step - loss: 3.0947 - accuracy: 0.3782 - val_loss: 36.4364 - val_accuracy: 0.0027\n",
      "Epoch 66/100\n",
      "195000/195000 [==============================] - 408s 2ms/step - loss: 3.0706 - accuracy: 0.3816 - val_loss: 36.2326 - val_accuracy: 0.0028\n",
      "Epoch 67/100\n",
      "195000/195000 [==============================] - 418s 2ms/step - loss: 3.0471 - accuracy: 0.3845 - val_loss: 36.8441 - val_accuracy: 0.0028\n",
      "Epoch 68/100\n",
      "195000/195000 [==============================] - 410s 2ms/step - loss: 3.0187 - accuracy: 0.3889 - val_loss: 36.8988 - val_accuracy: 0.0025\n",
      "Epoch 69/100\n",
      "195000/195000 [==============================] - 420s 2ms/step - loss: 2.9993 - accuracy: 0.3923 - val_loss: 36.9267 - val_accuracy: 0.0027\n",
      "Epoch 70/100\n",
      "195000/195000 [==============================] - 400s 2ms/step - loss: 2.9762 - accuracy: 0.3966 - val_loss: 37.5099 - val_accuracy: 0.0025\n",
      "Epoch 71/100\n",
      "195000/195000 [==============================] - 436s 2ms/step - loss: 2.9556 - accuracy: 0.3989 - val_loss: 37.3043 - val_accuracy: 0.0027\n",
      "Epoch 72/100\n",
      "195000/195000 [==============================] - 406s 2ms/step - loss: 2.9275 - accuracy: 0.4042 - val_loss: 37.9314 - val_accuracy: 0.0025\n",
      "Epoch 73/100\n",
      "195000/195000 [==============================] - 433s 2ms/step - loss: 2.9060 - accuracy: 0.4079 - val_loss: 38.3206 - val_accuracy: 0.0023\n",
      "Epoch 74/100\n",
      "195000/195000 [==============================] - 413s 2ms/step - loss: 2.8847 - accuracy: 0.4105 - val_loss: 38.7618 - val_accuracy: 0.0025\n",
      "Epoch 75/100\n",
      "195000/195000 [==============================] - 409s 2ms/step - loss: 2.8629 - accuracy: 0.4150 - val_loss: 39.0470 - val_accuracy: 0.0023\n",
      "Epoch 76/100\n",
      "195000/195000 [==============================] - 415s 2ms/step - loss: 2.8466 - accuracy: 0.4170 - val_loss: 39.1281 - val_accuracy: 0.0023\n",
      "Epoch 77/100\n",
      "195000/195000 [==============================] - 410s 2ms/step - loss: 2.8288 - accuracy: 0.4197 - val_loss: 39.7236 - val_accuracy: 0.0023\n",
      "Epoch 78/100\n",
      "195000/195000 [==============================] - 419s 2ms/step - loss: 2.8081 - accuracy: 0.4226 - val_loss: 39.7664 - val_accuracy: 0.0023\n",
      "Epoch 79/100\n",
      "195000/195000 [==============================] - 402s 2ms/step - loss: 2.7889 - accuracy: 0.4262 - val_loss: 39.8266 - val_accuracy: 0.0025\n",
      "Epoch 80/100\n",
      "195000/195000 [==============================] - 476s 2ms/step - loss: 2.7700 - accuracy: 0.4302 - val_loss: 40.4704 - val_accuracy: 0.0023\n",
      "Epoch 81/100\n",
      "195000/195000 [==============================] - 399s 2ms/step - loss: 2.7458 - accuracy: 0.4324 - val_loss: 40.8222 - val_accuracy: 0.0023\n",
      "Epoch 82/100\n",
      "195000/195000 [==============================] - 482s 2ms/step - loss: 2.7345 - accuracy: 0.4346 - val_loss: 41.0966 - val_accuracy: 0.0023\n",
      "Epoch 83/100\n",
      "195000/195000 [==============================] - 403s 2ms/step - loss: 2.7155 - accuracy: 0.4388 - val_loss: 41.4513 - val_accuracy: 0.0024\n",
      "Epoch 84/100\n",
      "195000/195000 [==============================] - 457s 2ms/step - loss: 2.6969 - accuracy: 0.4408 - val_loss: 41.4374 - val_accuracy: 0.0021\n",
      "Epoch 85/100\n",
      "195000/195000 [==============================] - 406s 2ms/step - loss: 2.6753 - accuracy: 0.4449 - val_loss: 41.5051 - val_accuracy: 0.0023\n",
      "Epoch 86/100\n",
      "195000/195000 [==============================] - 440s 2ms/step - loss: 2.6648 - accuracy: 0.4453 - val_loss: 41.8046 - val_accuracy: 0.0024\n",
      "Epoch 87/100\n",
      "195000/195000 [==============================] - 411s 2ms/step - loss: 2.6472 - accuracy: 0.4485 - val_loss: 42.3167 - val_accuracy: 0.0025\n",
      "Epoch 88/100\n",
      "195000/195000 [==============================] - 423s 2ms/step - loss: 2.6303 - accuracy: 0.4511 - val_loss: 42.3059 - val_accuracy: 0.0023\n",
      "Epoch 89/100\n",
      "195000/195000 [==============================] - 415s 2ms/step - loss: 2.6154 - accuracy: 0.4544 - val_loss: 42.4377 - val_accuracy: 0.0025\n",
      "Epoch 90/100\n",
      "195000/195000 [==============================] - 418s 2ms/step - loss: 2.5984 - accuracy: 0.4575 - val_loss: 42.7071 - val_accuracy: 0.0021\n",
      "Epoch 91/100\n",
      "195000/195000 [==============================] - 419s 2ms/step - loss: 2.5879 - accuracy: 0.4575 - val_loss: 43.1010 - val_accuracy: 0.0022\n",
      "Epoch 92/100\n",
      "195000/195000 [==============================] - 439s 2ms/step - loss: 2.5645 - accuracy: 0.4627 - val_loss: 43.3847 - val_accuracy: 0.0022\n",
      "Epoch 93/100\n",
      "195000/195000 [==============================] - 424s 2ms/step - loss: 2.5584 - accuracy: 0.4641 - val_loss: 43.3470 - val_accuracy: 0.0022\n",
      "Epoch 94/100\n",
      "195000/195000 [==============================] - 403s 2ms/step - loss: 2.5375 - accuracy: 0.4659 - val_loss: 43.4560 - val_accuracy: 0.0020\n",
      "Epoch 95/100\n",
      "195000/195000 [==============================] - 481s 2ms/step - loss: 2.5244 - accuracy: 0.4698 - val_loss: 43.7866 - val_accuracy: 0.0021\n",
      "Epoch 96/100\n",
      "195000/195000 [==============================] - 400s 2ms/step - loss: 2.5060 - accuracy: 0.4718 - val_loss: 44.1556 - val_accuracy: 0.0021\n",
      "Epoch 97/100\n",
      "195000/195000 [==============================] - 449s 2ms/step - loss: 2.4969 - accuracy: 0.4743 - val_loss: 43.6217 - val_accuracy: 0.0021\n",
      "Epoch 98/100\n",
      "195000/195000 [==============================] - 406s 2ms/step - loss: 2.4928 - accuracy: 0.4739 - val_loss: 43.8633 - val_accuracy: 0.0020\n",
      "Epoch 99/100\n",
      "195000/195000 [==============================] - 488s 3ms/step - loss: 2.4709 - accuracy: 0.4784 - val_loss: 44.6057 - val_accuracy: 0.0021\n",
      "Epoch 100/100\n",
      "195000/195000 [==============================] - 407s 2ms/step - loss: 2.4588 - accuracy: 0.4797 - val_loss: 44.5639 - val_accuracy: 0.0020\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 50, input_length=seq_length)) #create word embedding, better than BoW\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())\n",
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit model\n",
    "saver = CustomSaver()\n",
    "history = model.fit(X, y, callbacks=[saver], batch_size=128, epochs=100, validation_split=0.2)\n",
    " \n",
    "#model.save('model50.h5')\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#history.history['accuracy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
